{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e34a33-ecbd-4731-a1a5-18405b1736f7",
   "metadata": {},
   "source": [
    "# Try Setting up a New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0da01e-0ece-40e4-a38d-76a9fd29e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1058f2ad-ec19-4190-bceb-124bde99748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets.utils as dataset_utils\n",
    "\n",
    "from gendis.datasets import CausalMNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import normflows as nf\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from gendis.datasets import CausalMNIST, ClusteredMultiDistrDataModule\n",
    "from gendis.encoder import NonparametricClusteredCausalEncoder\n",
    "from gendis.model import NeuralClusteredASCMFlow\n",
    "from gendis.base import CausalMultiscaleFlow\n",
    "from gendis.normalizing_flow.distribution import NonparametricClusteredCausalDistribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdd0909-d915-4975-ac29-c10bd946c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list(x, n_clusters):\n",
    "    quotient = x // n_clusters\n",
    "    remainder = x % n_clusters\n",
    "    result = [quotient] * (n_clusters - 1)\n",
    "    result.append(quotient + remainder)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af66f152-ac7a-418a-bca5-5f285e5dfea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "\n",
      "\tsaving to results/chain-seed=1234-results.npz \n",
      "\n",
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with n_jobs: 1\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-0-train.pt\"\n",
      "torch.Size([3, 28, 28]) {'width': tensor([0.5344]), 'color': tensor([0.3977]), 'fracture_thickness': tensor([8.9378]), 'fracture_num_fractures': tensor([1.]), 'label': 0, 'intervention_targets': [0, 0, 0]}\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-1-train.pt\"\n",
      "torch.Size([3, 28, 28]) {'width': tensor([4.0281]), 'color': tensor([0.3975]), 'fracture_thickness': tensor([9.4378]), 'fracture_num_fractures': tensor([1.]), 'label': 0, 'intervention_targets': [1, 0, 0]}\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-2-train.pt\"\n",
      "torch.Size([3, 28, 28]) {'width': tensor([0.4431]), 'color': tensor([0.4416]), 'fracture_thickness': tensor([10.2519]), 'fracture_num_fractures': tensor([0.]), 'label': 0, 'intervention_targets': [0, 0, 1]}\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-3-train.pt\"\n",
      "torch.Size([3, 28, 28]) {'width': tensor([1.2834]), 'color': tensor([0.5551]), 'fracture_thickness': tensor([8.9974]), 'fracture_num_fractures': tensor([0.]), 'label': 0, 'intervention_targets': [0, 0, 1]}\n",
      "torch.Size([23692, 3, 28, 28]) 6 torch.Size([23692]) torch.Size([23692]) torch.Size([23692])\n"
     ]
    }
   ],
   "source": [
    "graph_type = \"chain\"\n",
    "adjacency_matrix = np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]])\n",
    "latent_dim = len(adjacency_matrix)\n",
    "results_dir = Path(\"./results/\")\n",
    "results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# root = \"/home/adam2392/projects/data/\"\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "# print(args)\n",
    "# root = args.root_dir\n",
    "seed = 1234\n",
    "max_epochs = 10\n",
    "accelerator = 'cpu'\n",
    "batch_size = 10\n",
    "log_dir = './'\n",
    "\n",
    "devices = 1\n",
    "n_jobs = 1\n",
    "num_workers = 2\n",
    "print(\"Running with n_jobs:\", n_jobs)\n",
    "\n",
    "# output filename for the results\n",
    "fname = results_dir / f\"{graph_type}-seed={seed}-results.npz\"\n",
    "\n",
    "# set up logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(f\"\\n\\n\\tsaving to {fname} \\n\")\n",
    "\n",
    "# set seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "pl.seed_everything(seed, workers=True)\n",
    "\n",
    "# set up transforms for each image to augment the dataset\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        nf.utils.Scale(255.0 / 256.0),  # normalize the pixel values\n",
    "        nf.utils.Jitter(1 / 256.0),    # apply random generation\n",
    "        torchvision.transforms.RandomRotation(350),  # get random rotations\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load dataset\n",
    "datasets = []\n",
    "intervention_targets_per_distr = []\n",
    "hard_interventions_per_distr = None\n",
    "num_distrs = 0\n",
    "for intervention_idx in [None, 1, 2, 3]:\n",
    "    dataset = CausalMNIST(\n",
    "        root=root,\n",
    "        graph_type=graph_type,\n",
    "        label=0,\n",
    "        download=True,\n",
    "        train=True,\n",
    "        n_jobs=None,\n",
    "        intervention_idx=intervention_idx,\n",
    "        transform=transform,\n",
    "    )\n",
    "    dataset.prepare_dataset(overwrite=False)\n",
    "    datasets.append(dataset)\n",
    "    num_distrs += 1\n",
    "    intervention_targets_per_distr.append(dataset.intervention_targets)\n",
    "\n",
    "# now we can wrap this in a pytorch lightning datamodule\n",
    "data_module = ClusteredMultiDistrDataModule(\n",
    "    datasets=datasets,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    intervention_targets_per_distr=intervention_targets_per_distr,\n",
    "    log_dir=log_dir,\n",
    "    flatten=False,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "27133cfc-63d0-48f2-8f10-4043fd0da99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for idx, img in enumerate(data_module.train_dataset):\n",
    "    \n",
    "    print(idx, len(img))\n",
    "    print(img[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776592b2-ec17-4b73-9f6e-50260569bf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "94b76690-4c70-4fba-8b7e-268951647782",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_flows = 3  # number of flows to use in nonlinear ICA model\n",
    "lr_scheduler = None\n",
    "lr_min = 0.0\n",
    "lr = 1e-6\n",
    "\n",
    "# Define the model\n",
    "net_hidden_dim = 128\n",
    "net_hidden_dim_cbn = 128\n",
    "net_hidden_layers = 3\n",
    "net_hidden_layers_cbn = 3\n",
    "fix_mechanisms = False\n",
    "\n",
    "graph = adjacency_matrix\n",
    "cluster_sizes = generate_list(784 * 3, 3)\n",
    "\n",
    "# 01: Define the causal base distribution with the graph\n",
    "causalq0 = NonparametricClusteredCausalDistribution(\n",
    "    adjacency_matrix=graph,\n",
    "    cluster_sizes=cluster_sizes,\n",
    "    intervention_targets_per_distr=intervention_targets_per_distr,\n",
    "    hard_interventions_per_distr=hard_interventions_per_distr,\n",
    "    fix_mechanisms=fix_mechanisms,\n",
    "    n_flows=n_flows,\n",
    "    n_hidden_dim=net_hidden_dim,\n",
    "    n_layers=net_hidden_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ef9dafe-41ff-43ec-a39a-f8d54846bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 1176 (24, 7, 7)\n",
      "12 1176 (6, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (3, 28, 28)\n",
    "channels = 3\n",
    "\n",
    "# Define flows\n",
    "L = 2\n",
    "K = 3\n",
    "n_dims = np.prod(input_shape)\n",
    "hidden_channels = 256\n",
    "split_mode = 'channel'\n",
    "scale = True\n",
    "\n",
    "stride_factor = 2\n",
    "\n",
    "# Set up flows, distributions and merge operations\n",
    "merges = []\n",
    "flows = []\n",
    "for i in range(L):\n",
    "    flows_ = []\n",
    "    for j in range(K):\n",
    "        n_chs = channels * 2 ** (L + 1 - i)\n",
    "        flows_ += [nf.flows.GlowBlock(n_chs, hidden_channels,\n",
    "                                     split_mode=split_mode, scale=scale)]\n",
    "    flows_ += [nf.flows.Squeeze()]\n",
    "    flows += [flows_]\n",
    "    if i > 0:\n",
    "        merges += [nf.flows.Merge()]\n",
    "        latent_shape = (input_shape[0] * stride_factor ** (L - i), input_shape[1] // stride_factor ** (L - i), \n",
    "                        input_shape[2] // stride_factor ** (L - i))\n",
    "    else:\n",
    "        latent_shape = (input_shape[0] * stride_factor ** (L + 1), input_shape[1] // stride_factor ** L, \n",
    "                        input_shape[2] //stride_factor ** L)\n",
    "    print(n_chs, np.prod(latent_shape), latent_shape)\n",
    "\n",
    "\n",
    "# 03: Define the final normalizing flow model\n",
    "# Construct flow model with the multiscale architecture\n",
    "encoder = CausalMultiscaleFlow(causalq0, flows, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cd504dc6-7d52-40a8-8150-930b9b166656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 22,910,208\n"
     ]
    }
   ],
   "source": [
    "def print_num_params(model):\n",
    "    num_params = sum([np.prod(p.shape) for p in model.parameters()])\n",
    "    print(\"Number of parameters: {:,}\".format(num_params))\n",
    "\n",
    "print_num_params(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c02b1d68-2aa8-4d72-b080-32c2b861405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 14, 14])\n",
      "torch.Size([1, 1176]) 0 1176\n",
      "torch.float32\n",
      "torch.Size([1, 3, 28, 28]) torch.Size([1])\n",
      "torch.Size([1, 2352])\n"
     ]
    }
   ],
   "source": [
    "# run a test to make sure this actually works\n",
    "rand_img = torch.arange(28*28*3, dtype=torch.float32).view(1, 3, 28, 28)\n",
    "out = encoder.forward(rand_img)\n",
    "print(rand_img.dtype)\n",
    "print(rand_img.shape, out.shape)\n",
    "print(encoder.inverse_and_log_det(rand_img)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7876ea23-e72f-4948-8dd5-521072c7dbd2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                       | 0/2145 [09:00<?, ?it/s]\n",
      "Epoch 0:   0%|                                                                                                       | 0/2145 [06:35<?, ?it/s]\n",
      "Epoch 0:   1%|█                                                                            | 29/2145 [06:07<7:27:00, 12.68s/it, loss=1.84e+06]\n",
      "Epoch 0:   0%|                                                                                                       | 0/2145 [01:28<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type                 | Params\n",
      "-------------------------------------------------\n",
      "0 | encoder | CausalMultiscaleFlow | 22.9 M\n",
      "-------------------------------------------------\n",
      "22.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.9 M    Total params\n",
      "91.641    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23692, 3, 28, 28]) 6 torch.Size([23692]) torch.Size([23692]) torch.Size([23692])\n",
      "Sanity Checking DataLoader 0:   0%|                                                                                     | 0/2 [00:00<?, ?it/s]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "torch.Size([10]) torch.Size([10, 3, 28, 28]) torch.Size([10]) torch.Size([10]) torch.Size([10]) torch.Size([10]) torch.Size([10])\n",
      "Sanity Checking DataLoader 0:  50%|██████████████████████████████████████▌                                      | 1/2 [00:00<00:00,  3.06it/s]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "torch.Size([10]) torch.Size([10, 3, 28, 28]) torch.Size([10]) torch.Size([10]) torch.Size([10]) torch.Size([10]) torch.Size([10])\n",
      "Epoch 0:   0%|                                                                                                       | 0/2145 [00:00<?, ?it/s]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|                                                                              | 1/2145 [00:01<1:10:25,  1.97s/it, loss=6.06e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|                                                                                | 2/2145 [00:02<45:07,  1.26s/it, loss=6.08e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|                                                                                | 3/2145 [00:03<38:42,  1.08s/it, loss=6.06e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▏                                                                               | 4/2145 [00:03<33:43,  1.06it/s, loss=6.04e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▏                                                                               | 5/2145 [00:04<30:36,  1.17it/s, loss=6.02e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▏                                                                               | 6/2145 [00:04<28:31,  1.25it/s, loss=6.03e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▎                                                                               | 7/2145 [00:05<27:00,  1.32it/s, loss=6.03e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▎                                                                               | 8/2145 [00:05<26:37,  1.34it/s, loss=6.03e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▎                                                                               | 9/2145 [00:06<26:22,  1.35it/s, loss=6.03e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   0%|▎                                                                              | 10/2145 [00:07<25:31,  1.39it/s, loss=6.03e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▍                                                                              | 11/2145 [00:07<25:27,  1.40it/s, loss=6.02e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▍                                                                              | 12/2145 [00:08<24:49,  1.43it/s, loss=6.02e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▍                                                                              | 13/2145 [00:08<24:21,  1.46it/s, loss=6.02e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▌                                                                              | 14/2145 [00:09<24:46,  1.43it/s, loss=6.01e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▌                                                                                 | 15/2145 [00:10<24:33,  1.45it/s, loss=6e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▌                                                                                 | 16/2145 [00:10<24:11,  1.47it/s, loss=6e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▋                                                                              | 17/2145 [00:11<24:12,  1.46it/s, loss=5.99e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▋                                                                              | 18/2145 [00:12<23:51,  1.49it/s, loss=5.99e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▋                                                                              | 19/2145 [00:12<23:39,  1.50it/s, loss=5.98e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▋                                                                              | 20/2145 [00:13<23:41,  1.50it/s, loss=5.97e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▊                                                                              | 21/2145 [00:14<23:37,  1.50it/s, loss=5.96e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▊                                                                              | 22/2145 [00:14<23:20,  1.52it/s, loss=5.94e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▊                                                                              | 23/2145 [00:15<23:07,  1.53it/s, loss=5.93e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▉                                                                              | 24/2145 [00:15<23:01,  1.54it/s, loss=5.92e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▉                                                                              | 25/2145 [00:16<22:50,  1.55it/s, loss=5.92e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▉                                                                               | 26/2145 [00:16<22:41,  1.56it/s, loss=5.9e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|▉                                                                              | 27/2145 [00:17<22:29,  1.57it/s, loss=5.89e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|█                                                                              | 28/2145 [00:17<22:32,  1.56it/s, loss=5.86e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|█                                                                              | 29/2145 [00:18<22:25,  1.57it/s, loss=5.85e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|█                                                                              | 30/2145 [00:18<22:16,  1.58it/s, loss=5.84e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|█▏                                                                             | 31/2145 [00:19<22:10,  1.59it/s, loss=5.83e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   1%|█▏                                                                             | 32/2145 [00:20<22:01,  1.60it/s, loss=5.82e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▏                                                                              | 33/2145 [00:20<21:55,  1.61it/s, loss=5.8e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▎                                                                              | 34/2145 [00:21<21:48,  1.61it/s, loss=5.8e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▎                                                                             | 35/2145 [00:21<21:41,  1.62it/s, loss=5.78e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▎                                                                             | 36/2145 [00:22<21:34,  1.63it/s, loss=5.77e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▎                                                                             | 37/2145 [00:22<21:29,  1.64it/s, loss=5.76e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▍                                                                             | 38/2145 [00:23<21:23,  1.64it/s, loss=5.75e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▍                                                                             | 39/2145 [00:23<21:17,  1.65it/s, loss=5.74e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▍                                                                             | 40/2145 [00:24<21:20,  1.64it/s, loss=5.74e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▌                                                                             | 41/2145 [00:25<21:42,  1.62it/s, loss=5.73e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▌                                                                             | 42/2145 [00:26<21:53,  1.60it/s, loss=5.72e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▌                                                                             | 43/2145 [00:26<21:49,  1.61it/s, loss=5.71e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▋                                                                              | 44/2145 [00:27<21:43,  1.61it/s, loss=5.7e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▋                                                                             | 45/2145 [00:27<21:37,  1.62it/s, loss=5.68e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▋                                                                             | 46/2145 [00:28<21:34,  1.62it/s, loss=5.68e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▋                                                                             | 47/2145 [00:28<21:29,  1.63it/s, loss=5.67e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▊                                                                             | 48/2145 [00:29<21:22,  1.63it/s, loss=5.67e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▊                                                                             | 49/2145 [00:29<21:17,  1.64it/s, loss=5.65e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▊                                                                             | 50/2145 [00:30<21:12,  1.65it/s, loss=5.64e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▉                                                                             | 51/2145 [00:30<21:06,  1.65it/s, loss=5.63e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▉                                                                             | 52/2145 [00:31<21:02,  1.66it/s, loss=5.63e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   2%|█▉                                                                             | 53/2145 [00:31<20:57,  1.66it/s, loss=5.62e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   3%|██                                                                              | 54/2145 [00:32<21:05,  1.65it/s, loss=5.6e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   3%|██                                                                              | 55/2145 [00:33<21:27,  1.62it/s, loss=5.6e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   3%|██                                                                             | 56/2145 [00:34<21:36,  1.61it/s, loss=5.58e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   3%|██                                                                             | 57/2145 [00:36<22:02,  1.58it/s, loss=5.57e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   3%|██▏                                                                            | 58/2145 [00:36<22:04,  1.58it/s, loss=5.57e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n",
      "Epoch 0:   3%|██▏                                                                            | 59/2145 [00:37<22:02,  1.58it/s, loss=5.56e+04]torch.Size([10, 6, 14, 14])\n",
      "torch.Size([10, 1176]) 0 1176\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'gendis.model.NeuralClusteredASCMFlow'>: it's not the same object as gendis.model.NeuralClusteredASCMFlow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     38\u001b[0m     model,\n\u001b[1;32m     39\u001b[0m     datamodule\u001b[38;5;241m=\u001b[39mdata_module,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# save the final model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/serialization.py:840\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    838\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    839\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 840\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    842\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'gendis.model.NeuralClusteredASCMFlow'>: it's not the same object as gendis.model.NeuralClusteredASCMFlow"
     ]
    }
   ],
   "source": [
    "# 04a: Define now the full pytorch lightning model\n",
    "model = NeuralClusteredASCMFlow(\n",
    "    encoder=encoder,\n",
    "    lr=lr,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    lr_min=lr_min,\n",
    ")\n",
    "\n",
    "# 04b: Define the trainer for the model\n",
    "checkpoint_root_dir = f\"{graph_type}-seed={seed}\"\n",
    "checkpoint_dir = Path(checkpoint_root_dir)\n",
    "checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
    "logger = None\n",
    "wandb = False\n",
    "check_val_every_n_epoch = 1\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,\n",
    "    save_top_k=5,\n",
    "    monitor=\"train_loss\",\n",
    "    every_n_epochs=check_val_every_n_epoch,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    logger=logger,\n",
    "    devices=devices,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    check_val_every_n_epoch=check_val_every_n_epoch,\n",
    "    accelerator=accelerator,\n",
    ")\n",
    "\n",
    "# 05: Fit the model and save the data\n",
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=data_module,\n",
    ")\n",
    "\n",
    "# save the final model\n",
    "torch.save(model, checkpoint_dir / \"model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b5272838-a8a8-48d2-a861-1fc39222927f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'gendis.model.NeuralClusteredASCMFlow'>: it's not the same object as gendis.model.NeuralClusteredASCMFlow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save the final model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/serialization.py:840\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    838\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    839\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 840\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    842\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'gendis.model.NeuralClusteredASCMFlow'>: it's not the same object as gendis.model.NeuralClusteredASCMFlow"
     ]
    }
   ],
   "source": [
    "# save the final model\n",
    "torch.save(model, checkpoint_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417ec2e-32c5-4e96-b849-6910e82b6718",
   "metadata": {},
   "source": [
    "## Let's sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760b03c-1e84-4e0c-b9b1-28077f7856e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(batch_size, n_chs, width, height), (batch_size, n_chs, width, height), ..., repeat for n_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65610e64-7463-4aba-bc12-5d8dbbc4a224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e645aa3-d4ab-44dd-871a-6fa1f150c826",
   "metadata": {},
   "source": [
    "## RealNVP with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "78411439-862b-4413-a6b9-481647dc236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b550bfce-d7e8-47c8-bb27-fd1ef24c4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f664611-c2d3-4ff5-a546-ad5dfd8feded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Adapted) Code from PyTorch's Resnet impl: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            # norm_layer = nn.BatchNorm2d\n",
    "            norm_layer = nn.InstanceNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            #norm_layer = nn.BatchNorm2d\n",
    "            norm_layer = nn.InstanceNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "04b53c46-32be-4368-b610-3a6975006b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchNorm2d(nn.modules.batchnorm._NormBase):\n",
    "    ''' Partially based on: \n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        https://discuss.pytorch.org/t/implementing-batchnorm-in-pytorch-problem-with-updating-self-running-mean-and-self-running-var/49314/5 \n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        eps=1e-5,\n",
    "        momentum=0.005,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype, 'affine': False, 'track_running_stats': True}\n",
    "        super(MyBatchNorm2d, self).__init__(\n",
    "            num_features, eps, momentum, **factory_kwargs\n",
    "        )\n",
    "        \n",
    "    def _check_input_dim(self, input):\n",
    "        if input.dim() != 4:\n",
    "            raise ValueError(\"expected 4D input (got {}D input)\".format(input.dim()))\n",
    "\n",
    "    def forward(self, input, validation=False):\n",
    "        self._check_input_dim(input)\n",
    "    \n",
    "        if self.training:\n",
    "            # Note: Need to detatch `running_{mean,var}` so don't backwards propagate through them\n",
    "            unbiased_var, tmean = torch.var_mean(input, [0, 2, 3], unbiased=True)\n",
    "            mean = torch.mean(input, [0, 2, 3]) # along channel axis\n",
    "            unbiased_var = torch.var(input, [0, 2, 3], unbiased=True) # along channel axis\n",
    "            running_mean = (1.0 - self.momentum) * self.running_mean.detach() + self.momentum * mean\n",
    "            \n",
    "            # Strange: PyTorch impl. of running variance uses biased_variance for the batch calc but\n",
    "            # *unbiased_var* for the running_var!\n",
    "            # https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Normalization.cpp#L190\n",
    "            running_var = (1.0 - self.momentum) * self.running_var.detach() + self.momentum * unbiased_var\n",
    "            \n",
    "            # BK: Modification from the paper to use running mean/var instead of batch mean/var\n",
    "            # change shape\n",
    "            current_mean = running_mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            current_var = running_var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            \n",
    "            denom = (current_var + self.eps)\n",
    "            y = (input - current_mean) / denom.sqrt()\n",
    "            \n",
    "            self.running_mean = running_mean\n",
    "            self.running_var = running_var\n",
    "            \n",
    "            return y, -0.5 * torch.log(denom)\n",
    "        else:\n",
    "            current_mean = self.running_mean.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            current_var = self.running_var.view([1, self.num_features, 1, 1]).expand_as(input)\n",
    "            \n",
    "            if validation:\n",
    "                denom = (current_var + self.eps)\n",
    "                y = (input - current_mean) / denom.sqrt()\n",
    "            else:\n",
    "                # Reverse operation for testing\n",
    "                denom = (current_var + self.eps)\n",
    "                y = input * denom.sqrt() + current_mean\n",
    "                \n",
    "            return y, -0.5 * torch.log(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95f0f3c1-9fe9-4256-be1e-dd5176d690ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = tuple([-1] + list(shape))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.reshape(x, self.shape)\n",
    "\n",
    "def dense_backbone(shape, network_width):\n",
    "    input_width = shape[0] * shape[1] * shape[2]\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(input_width, network_width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(network_width, input_width),\n",
    "        Reshape(shape)\n",
    "    )\n",
    "\n",
    "def bottleneck_backbone(in_planes, planes):\n",
    "    return nn.Sequential(\n",
    "        conv3x3(in_planes, planes),\n",
    "        BasicBlock(planes, planes),\n",
    "        BasicBlock(planes, planes),\n",
    "        conv3x3(planes, in_planes),\n",
    "    )\n",
    "\n",
    "check_mask = {}\n",
    "check_mask_device = {}\n",
    "def checkerboard_mask(shape, to_device=True):\n",
    "    global check_mask, check_mask_device\n",
    "    if shape not in check_mask:\n",
    "        check_mask[shape] = 1 - np.indices(shape).sum(axis=0) % 2\n",
    "        check_mask[shape] = torch.Tensor(check_mask[shape])\n",
    "        \n",
    "    if to_device and shape not in check_mask_device:\n",
    "        check_mask_device[shape] = check_mask[shape].to(device)\n",
    "        \n",
    "    return check_mask_device[shape] if to_device else check_mask[shape]\n",
    "\n",
    "chan_mask = {}\n",
    "chan_mask_device = {}\n",
    "def channel_mask(shape, to_device=True):\n",
    "    assert len(shape) == 3, shape\n",
    "    assert shape[0] % 2 == 0, shape\n",
    "    global chan_mask, chan_mask_device\n",
    "    if shape not in chan_mask:\n",
    "        chan_mask[shape] = torch.cat([torch.zeros((shape[0] // 2, shape[1], shape[2])),\n",
    "                                      torch.ones((shape[0] // 2, shape[1], shape[2])),],\n",
    "                                      dim=0)\n",
    "        assert chan_mask[shape].shape == shape, (chan_mask[shape].shape, shape)\n",
    "        \n",
    "    if to_device and shape not in chan_mask_device:\n",
    "        chan_mask_device[shape] = chan_mask[shape].to(device)\n",
    "        \n",
    "    return chan_mask_device[shape] if to_device else chan_mask[shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d75a6589-94cc-4d91-9149-b8dd07c56abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlowMNist(nn.Module):\n",
    "    EPSILON = 1e-5\n",
    "    \n",
    "    def __init__(self, num_coupling=6, num_final_coupling=4, planes=64):\n",
    "        super(NormalizingFlowMNist, self).__init__()\n",
    "        self.num_coupling = num_coupling\n",
    "        self.num_final_coupling = num_final_coupling\n",
    "        self.shape = (3, 28, 28)\n",
    "        \n",
    "        self.planes = planes\n",
    "        self.s = nn.ModuleList()\n",
    "        self.t = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        # Learnable scalar scaling parameters for outputs of S and T\n",
    "        self.s_scale = nn.ParameterList()\n",
    "        self.t_scale = nn.ParameterList()\n",
    "        self.t_bias = nn.ParameterList()\n",
    "        self.shapes = []\n",
    "      \n",
    "        shape = self.shape\n",
    "        for i in range(num_coupling):\n",
    "            self.s.append(bottleneck_backbone(shape[0], planes))\n",
    "            self.t.append(bottleneck_backbone(shape[0], planes))\n",
    "            \n",
    "            self.s_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_bias.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            \n",
    "            self.norms.append(MyBatchNorm2d(shape[0]))\n",
    "            \n",
    "            self.shapes.append(shape)\n",
    "           \n",
    "            if i % 6 == 2:\n",
    "                shape = (4 * shape[0], shape[1] // 2, shape[2] // 2)\n",
    "                \n",
    "            if i % 6 == 5:\n",
    "                # Factoring out half the channels\n",
    "                shape = (shape[0] // 2, shape[1], shape[2])\n",
    "                planes = 2 * planes\n",
    "       \n",
    "        # Final coupling layers checkerboard\n",
    "        for i in range(num_final_coupling):\n",
    "            self.s.append(bottleneck_backbone(shape[0], planes))\n",
    "            self.t.append(bottleneck_backbone(shape[0], planes))\n",
    "            \n",
    "            self.s_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_scale.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            self.t_bias.append(torch.nn.Parameter(torch.zeros(shape), requires_grad=True))\n",
    "            \n",
    "            self.norms.append(MyBatchNorm2d(shape[0]))\n",
    "            \n",
    "            self.shapes.append(shape)\n",
    "           \n",
    "        self.validation = False\n",
    "    \n",
    "    def validate(self):\n",
    "        self.eval()\n",
    "        self.validation = True\n",
    "        \n",
    "    def train(self, mode=True):\n",
    "        nn.Module.train(self, mode)\n",
    "        self.validation = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training or self.validation:\n",
    "            s_vals = []\n",
    "            norm_vals = []\n",
    "            y_vals = []\n",
    "            \n",
    "            for i in range(self.num_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape) if i % 6 < 3 else channel_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "               \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * x) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * x))\n",
    "                y = mask * x + (1 - mask) * (x * torch.exp(s) + t)\n",
    "                s_vals.append(torch.flatten((1 - mask) * s))\n",
    "               \n",
    "                if self.norms[i] is not None:\n",
    "                    y, norm_loss = self.norms[i](y, validation=self.validation)\n",
    "                    norm_vals.append(norm_loss)\n",
    "                    \n",
    "                if i % 6 == 2:\n",
    "                    y = torch.nn.functional.pixel_unshuffle(y, 2)\n",
    "                    \n",
    "                if i % 6 == 5:\n",
    "                    factor_channels = y.shape[1] // 2\n",
    "                    y_vals.append(torch.flatten(y[:, factor_channels:, :, :], 1))\n",
    "                    y = y[:, :factor_channels, :, :]\n",
    "                    \n",
    "                x = y\n",
    "                \n",
    "            # Final checkboard coupling\n",
    "            for i in range(self.num_coupling, self.num_coupling + self.num_final_coupling):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "               \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * x) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * x))\n",
    "                y = mask * x + (1 - mask) * (x * torch.exp(s) + t)\n",
    "                s_vals.append(torch.flatten((1 - mask) * s))\n",
    "                \n",
    "                if self.norms[i] is not None:\n",
    "                    y, norm_loss = self.norms[i](y, validation=self.validation)\n",
    "                    norm_vals.append(norm_loss)\n",
    "                \n",
    "                x = y\n",
    "\n",
    "            y_vals.append(torch.flatten(y, 1))\n",
    "            \n",
    "            # Return outputs and vars needed for determinant\n",
    "            return (torch.flatten(torch.cat(y_vals, 1), 1),\n",
    "                    torch.cat(s_vals), \n",
    "                    torch.cat([torch.flatten(v) for v in norm_vals]) if len(norm_vals) > 0 else torch.zeros(1),\n",
    "                    torch.cat([torch.flatten(s) for s in self.s_scale]))\n",
    "        else:\n",
    "            y = x\n",
    "            y_remaining = y\n",
    "           \n",
    "            layer_vars = np.prod(self.shapes[-1])\n",
    "            y = torch.reshape(y_remaining[:, -layer_vars:], (-1,) + self.shapes[-1])\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "            \n",
    "            # Reversed final checkboard coupling\n",
    "            for i in reversed(range(self.num_coupling, self.num_coupling + self.num_final_coupling)):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "                \n",
    "                if self.norms[i] is not None:\n",
    "                    y, _ = self.norms[i](y)\n",
    "              \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * y) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * y))\n",
    "                x = mask * y + (1 - mask) * ((y - t) * torch.exp(-s))\n",
    "               \n",
    "                y = x           \n",
    "          \n",
    "            layer_vars = np.prod(shape)\n",
    "            y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "            y_remaining = y_remaining[:, :-layer_vars]\n",
    "            \n",
    "            # Multi-scale coupling layers\n",
    "            for i in reversed(range(self.num_coupling)):\n",
    "                shape = self.shapes[i]\n",
    "                mask = checkerboard_mask(shape) if i % 6 < 3 else channel_mask(shape)\n",
    "                mask = mask if i % 2 == 0 else (1 - mask)\n",
    "              \n",
    "                if self.norms[i] is not None:\n",
    "                    y, _ = self.norms[i](y)\n",
    "                    \n",
    "                t = (self.t_scale[i]) * self.t[i](mask * y) + (self.t_bias[i])\n",
    "                s = (self.s_scale[i]) * torch.tanh(self.s[i](mask * y))\n",
    "                x = mask * y + (1 - mask) * ((y - t) * torch.exp(-s))\n",
    "               \n",
    "                if i % 6 == 3:\n",
    "                    x = torch.nn.functional.pixel_shuffle(x, 2)\n",
    "                    \n",
    "                y = x\n",
    "                \n",
    "                if i > 0 and i % 6 == 0:\n",
    "                    layer_vars = np.prod(shape)\n",
    "                    y = torch.cat((y, torch.reshape(y_remaining[:, -layer_vars:], (-1,) + shape)), 1)\n",
    "                    y_remaining = y_remaining[:, :-layer_vars]\n",
    "            \n",
    "            assert np.prod(y_remaining.shape) == 0\n",
    "            \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5137fc93-6114-491c-8964-52e0fe2c8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(x):\n",
    "    # Convert back to integer values\n",
    "    x = x * 255.\n",
    "    \n",
    "    # Add random uniform [0, 1] noise to get a proper likelihood estimate\n",
    "    # https://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/\n",
    "    x = x + torch.rand(x.shape)\n",
    "\n",
    "    # Apply transform to deal with boundary effects (see realNVP paper)\n",
    "    #x = torch.logit(0.05 + 0.90 * x / 256)\n",
    "    #return x\n",
    "    return x / 255\n",
    "\n",
    "def post_process(x):\n",
    "    # Convert back to integer values\n",
    "    #return torch.clip(torch.floor(256 / 0.90 * (torch.sigmoid(x) - 0.05)), min=0, max=255) / 255\n",
    "    return torch.clip(x, min=0, max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07cc4da5-b13b-41df-b473-a3a00ab8be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NormalizingFlowMNist(num_coupling=12, num_final_coupling=4, planes=64).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c950097-7f4b-45e1-bb61-b64f059997ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rand_img[0, ...].shape)\n",
    "model(rand_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6fa83c78-a5ad-4f7d-a17f-46af110ca0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('data', train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                               ]))\n",
    "\n",
    "batch_size = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "01a3c4d8-e45b-4394-bcb1-c2855176d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, _) in enumerate(train_loader):\n",
    "    # Transfer to GPU\n",
    "    X = pre_process(X)\n",
    "    X = X.to(device)\n",
    "\n",
    "    # Compute prediction and loss\n",
    "    print(X.shape)\n",
    "    y, s, norms, scale = model(X)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e08dd-a5de-4feb-9e16-44a7beea11d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdrl",
   "language": "python",
   "name": "cdrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
