{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7abc5c9c-f9ee-4cbe-8e17-64fba01626c3",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "In our analysis, we want to generate a variation of the MNIST dataset that demonstrates the settings in our paper.\n",
    "First we will consider a setting with only one type of digit (i.e. a 0). We will then demonstrate how to generate\n",
    "data according to a latent causal graph over the considered variables.\n",
    "More specifically, consider the following continuous variables:\n",
    "\n",
    "- color\n",
    "- thickness/thinness (width)\n",
    "- fractures\n",
    "- swelling\n",
    "\n",
    "Each of these will be continuously generated within a compact domain according to the causal graph:\n",
    "\n",
    "swelling <- width <- color -> fractures\n",
    "\n",
    "(also shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831ce63e-e1bc-4156-9ad6-9d1137975556",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 11.0.0 (20240428.1522)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"186pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 186.30 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 182.3,-112 182.3,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"42.14\" cy=\"-90\" rx=\"42.14\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"42.14\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">swelling</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"42.14\" cy=\"-18\" rx=\"31.9\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"42.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">width</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M42.14,-71.7C42.14,-64.41 42.14,-55.73 42.14,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"45.64,-47.62 42.14,-37.62 38.64,-47.62 45.64,-47.62\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"133.14\" cy=\"-90\" rx=\"29.86\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.14\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">color</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;B -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>C&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.16,-75.17C102.03,-65.07 84.01,-51.21 69.16,-39.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.65,-37.29 61.59,-33.96 67.38,-42.83 71.65,-37.29\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135.14\" cy=\"-18\" rx=\"43.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">fractures</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.63,-71.7C133.84,-64.41 134.09,-55.73 134.32,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.82,-47.71 134.61,-37.62 130.82,-47.51 137.82,-47.71\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x107e727a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph()\n",
    "\n",
    "# Add nodes\n",
    "dot.node('A', 'swelling')\n",
    "dot.node('B', 'width')\n",
    "dot.node('C', 'color')\n",
    "dot.node('D', 'fractures')\n",
    "\n",
    "# Add edges\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('C', 'B')\n",
    "dot.edge('C', 'D')\n",
    "\n",
    "# Render the graph\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2eb58-cae7-4ee7-a386-602c665c17aa",
   "metadata": {},
   "source": [
    "where the degree of swelling is inversely correlated with the width of the digit, which has a relationship with the color. The more red an image is the more wide they are, and the more fractures they have.\n",
    "\n",
    "Now, let us actually generate the data in steps. We will leverage the MORPHO-MNIST API to generate different widths, fractures and swelling.\n",
    "To modify the MNIST dataset to provide color, we will implement a variation of the IRM paper procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19027ec0-bca6-4776-b368-bec88bd82628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b4a329-bd63-4b59-b4b4-02b6d534660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets.utils as dataset_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a022180-72c6-4eba-91a4-3c441120b5e4",
   "metadata": {},
   "source": [
    "## Generate Images with different colors on the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f14ed4-18d0-4204-ab5d-0f181450a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, color_value):\n",
    "    \"\"\"Converts grayscale image to a uniform color based on a continuous value\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    colored_arr = np.zeros((h, w, 3), dtype=dtype)\n",
    "    color = plt.cm.viridis(color_value)[:3]  # Use a continuous colormap like viridis and extract RGB values\n",
    "    mask = arr > 0  # Mask to identify the digit\n",
    "    for i in range(3):\n",
    "        colored_arr[:, :, i][mask] = color[i] * 255  # Apply uniform color to the digit\n",
    "    return colored_arr.astype(dtype)\n",
    "\n",
    "def prepare_colored_mnist(root='./data', overwrite=False):\n",
    "    colored_mnist_dir = os.path.join(root, 'ColoredMNIST')\n",
    "    if not overwrite and os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
    "            and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
    "            and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
    "        print('Colored MNIST dataset already exists')\n",
    "        return\n",
    "\n",
    "    print('Preparing Colored MNIST')\n",
    "    train_mnist = datasets.MNIST(root, train=True, download=True)\n",
    "\n",
    "    train1_set = []\n",
    "    train2_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for idx, (im, label) in enumerate(train_mnist):\n",
    "        if idx % 10000 == 0:\n",
    "            print(f'Converting image {idx}/{len(train_mnist)}')\n",
    "        im_array = np.array(im)\n",
    "\n",
    "        # Normalize label to [0, 1] for continuous coloring\n",
    "        color_value = label / 9.0\n",
    "        \n",
    "#         print(idx, label,color_value)\n",
    "        colored_arr = color_grayscale_arr(im_array, color_value)\n",
    "        \n",
    "        assert False\n",
    "        if idx < 20000:\n",
    "            train1_set.append((Image.fromarray(colored_arr), label))\n",
    "        elif idx < 40000:\n",
    "            train2_set.append((Image.fromarray(colored_arr), label))\n",
    "        else:\n",
    "            test_set.append((Image.fromarray(colored_arr), label))\n",
    "\n",
    "    os.makedirs(colored_mnist_dir, exist_ok=True)\n",
    "    torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
    "    torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "    torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))\n",
    "\n",
    "def load_colored_mnist(env='train1', root='./data'):\n",
    "    colored_mnist_dir = os.path.join(root, 'ColoredMNIST')\n",
    "    print(f'Loading data from colored mnist: {colored_mnist_dir}')\n",
    "    if env in ['train1', 'train2', 'test']:\n",
    "        data_label_tuples = torch.load(os.path.join(colored_mnist_dir, env) + '.pt')\n",
    "    elif env == 'all_train':\n",
    "        data_label_tuples = torch.load(os.path.join(colored_mnist_dir, 'train1.pt')) + \\\n",
    "                            torch.load(os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "    else:\n",
    "        raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
    "    return data_label_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a5a1e-914a-4e74-863f-c1c8dfd8cadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = '/Users/adam2392/pytorch_data/'\n",
    "overwrite = True\n",
    "\n",
    "# Prepare Colored MNIST dataset\n",
    "prepare_colored_mnist(root, overwrite=overwrite)\n",
    "\n",
    "# Load some example images\n",
    "data_label_tuples = load_colored_mnist(env='train1', root=root)\n",
    "\n",
    "# Display some example images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, (img, label) in enumerate(data_label_tuples[:5]):\n",
    "    print(np.array(img).shape)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'Label: {label}')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde9e1-724b-4a3d-a9fe-2ea853828acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,1]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 1.5))\n",
    "img = plt.imshow(a, cmap=\"viridis\")\n",
    "plt.gca().set_visible(False)\n",
    "cax = plt.axes([0.1, 0.2, 0.8, 0.6])\n",
    "plt.colorbar(orientation=\"horizontal\", cax=cax, label='Possible colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6a054-839d-47ed-9c0c-b777094cbf33",
   "metadata": {},
   "source": [
    "## Generate images with different widths using MorphoMNIST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94449988-0afb-4a47-aa4d-0807309748a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gendis.datasets.morphomnist import perturb, morpho\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd2b47-6d67-4415-a00a-32837b26d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_perturbation(image, perturbation):\n",
    "    # Convert image to binary\n",
    "    image = image.squeeze().numpy()\n",
    "    binary_image = np.array(image) > 0\n",
    "#     binary_image = np.array(image.convert('L')) > 0  # Ensure the image is in grayscale and binary\n",
    "\n",
    "    morph = morpho.ImageMorphology(binary_image)\n",
    "    perturbed_image = perturbation(morph)\n",
    "    return Image.fromarray((perturbed_image * 255).astype(np.uint8))\n",
    "\n",
    "# Define the dataset loader with perturbations\n",
    "class PerturbedMNIST(datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, perturbation=None):\n",
    "        super(PerturbedMNIST, self).__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        self.perturbation = perturbation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(PerturbedMNIST, self).__getitem__(index)\n",
    "        if self.perturbation is not None:\n",
    "            img = apply_perturbation(img, self.perturbation)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        prepare_colored_mnist(root='', overwrite=False)\n",
    "    \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bed97-c65d-4f4e-a2cd-7d6ccba28eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the perturbations\n",
    "thinning = perturb.Thinning(amount=0.7)\n",
    "thickening = perturb.Thickening(amount=1.0)\n",
    "swelling = perturb.Swelling(strength=3, radius=10)\n",
    "fracture = perturb.Fracture(thickness=1.5, prune=2, num_frac=1)\n",
    "\n",
    "# Apply transformations and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "train_dataset_thinning = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=thinning)\n",
    "train_dataset_thickening = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=thickening)\n",
    "train_dataset_swelling = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=swelling)\n",
    "train_dataset_fracture = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=fracture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dbf2a-b88c-4a22-8937-f807dabe66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize some examples\n",
    "def visualize_examples(dataset, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i in range(5):\n",
    "        img, label = dataset[i]\n",
    "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display examples\n",
    "# visualize_examples(train_dataset_thinning, 'Thinning Perturbation')\n",
    "# visualize_examples(train_dataset_thickening, 'Thickening Perturbation')\n",
    "# visualize_examples(train_dataset_swelling, 'Swelling Perturbation')\n",
    "visualize_examples(train_dataset_fracture, 'Fracture Perturbation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2ea65-ed9c-4fac-a7f2-01451dd03f6e",
   "metadata": {},
   "source": [
    "## Combining Color and Morpho MNIST\n",
    "\n",
    "First, we'll look at how we can combine perturbations in a VisionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75c882-fb04-4359-b556-436bab98b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_perturbation(image, perturbation):\n",
    "    # Convert image to binary\n",
    "    image = np.array(image)\n",
    "    binary_image = np.array(image) > 0\n",
    "\n",
    "    morph = morpho.ImageMorphology(binary_image)\n",
    "    perturbed_image = perturbation(morph)\n",
    "    return Image.fromarray((perturbed_image * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "# Define the dataset loader with perturbations\n",
    "class PerturbedMNIST(datasets.MNIST):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "        perturbations=None,\n",
    "        \n",
    "    ):\n",
    "        super(PerturbedMNIST, self).__init__(\n",
    "            root,\n",
    "            train=train,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            download=download,\n",
    "        )\n",
    "\n",
    "        if perturbations is not None and not isinstance(perturbations, list):\n",
    "            raise RuntimeError(\"Should be a list of perturbations\")\n",
    "        self.perturbations = perturbations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(PerturbedMNIST, self).__getitem__(index)\n",
    "        img = img.squeeze()\n",
    "        if self.perturbations is not None:\n",
    "            for perturbation in self.perturbations:\n",
    "                img = torch.Tensor(np.array(img))\n",
    "                img = apply_perturbation(img, perturbation)\n",
    "        \n",
    "        # Normalize label to [0, 1] for continuous coloring\n",
    "        color_value = target / 9.0\n",
    "        img = color_grayscale_arr(np.array(img), color_value)\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    \n",
    "def color_grayscale_arr(arr, color_value):\n",
    "    \"\"\"Converts grayscale image to a uniform color based on a continuous value\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    colored_arr = np.zeros((h, w, 3), dtype=dtype)\n",
    "    color = plt.cm.viridis(color_value)[:3]  # Use a continuous colormap like viridis and extract RGB values\n",
    "    mask = arr > 0  # Mask to identify the digit\n",
    "    for i in range(3):\n",
    "        colored_arr[:, :, i][mask] = color[i] * 255  # Apply uniform color to the digit\n",
    "    return colored_arr.astype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca9411-db94-4b54-b2a6-9ddb147c1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize some examples\n",
    "def visualize_examples(dataset, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i in range(5):\n",
    "        img, label = dataset[i]\n",
    "#         print('here...',\n",
    "#               np.array(img).shape)\n",
    "#         img = np.moveaxis(np.array(img), 0, 2)\n",
    "#         print(img.shape)\n",
    "        img = Image.fromarray(img)\n",
    "        axes[i].imshow(img, cmap='viridis')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "perturbations = [\n",
    "    perturb.Thickening(amount=0.2),\n",
    "#     perturb.Swelling(strength=1.75, radius=7),\n",
    "    perturb.Fracture(thickness=1.1, \n",
    "#                      prune=10, \n",
    "                     num_frac=2)\n",
    "]\n",
    "    \n",
    "# Apply transformations and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "train_dataset = PerturbedMNIST(root=root, train=True, download=False, transform=transform, perturbations=perturbations)\n",
    "    \n",
    "visualize_examples(train_dataset, 'All perturbations and colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3301a-ca18-46b5-9c58-aee6fa08f9e7",
   "metadata": {},
   "source": [
    "## Full CausalMNIST on a single digit\n",
    "\n",
    "Now, we can demonstrate CausalMNIST on a single digit \"0\", where we apply color, fracture and width changes in the form of a latent causal graph among these latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0b9c40-0709-41da-bb68-87eafa706e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gendis.datasets.causalmnist import CausalMNIST\n",
    "from gendis.model import NonlinearNeuralClusteredASCMFlow, LinearNeuralClusteredASCMFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c375fe-88f0-42a4-8d0e-7f313d2e2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = 'chain'\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "\n",
    "dataset = CausalMNIST(root=root, graph_type=graph_type, label=0, download=True, train=True, n_jobs=None, intervention_idx=)\n",
    "dataset.prepare_dataset(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab69d72-9256-4d38-b024-01403307e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.prepare_dataset(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915575a9-cdfb-488d-bd82-c39f6c21db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in range(5):\n",
    "    img, meta_label = dataset[idx]\n",
    "    print(meta_label)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img, cmap='viridis')\n",
    "    ax.set_title(f\"Label: {meta_label}\")\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884afcf9-5214-49b1-bb6e-03e6fbb82307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numValues = 10000\n",
    "maxValue = 2.0\n",
    "skewness = -6   #Negative values are left skewed, positive values are right skewed.\n",
    "\n",
    "random = skewnorm.rvs(a = skewness / 2,loc=maxValue, \n",
    "                      scale=0.2,\n",
    "                      size=numValues)  #Skewnorm function\n",
    "\n",
    "random = random - min(random)      #Shift the set so the minimum value is equal to zero.\n",
    "random = random / max(random) * 2      #Standadize all the vlues between 0 and 1. \n",
    "# random = random * maxValue         #Multiply the standardized values by the maximum value.\n",
    "\n",
    "#Plot histogram to check skewness\n",
    "plt.hist(random,30,density=True, color = 'red', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac779126-ed9c-468a-bd22-0491744e2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "numValues = 10000\n",
    "maxValue = 10\n",
    "skewness = 1   #Negative values are left skewed, positive values are right skewed.\n",
    "\n",
    "width_intervention = stats.skewnorm.rvs(a=0, loc=2, size=(n_samples, 1))\n",
    "width_intervention = width_intervention - min(width_intervention)      #Shift the set so the minimum value is equal to zero.\n",
    "width = (torch.rand(size=(n_samples, 1)) * 2) * width_intervention\n",
    "# width = width - min(width)\n",
    "# width = width / max(width) * 2\n",
    "# random = random * maxValue         #Multiply the standardized values by the maximum value.\n",
    "width = width.squeeze()\n",
    "print(width.shape)\n",
    "#Plot histogram to check skewness\n",
    "plt.hist(width,30,density=True, \n",
    "#          color = 'red', \n",
    "         alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efed4d-e814-43b3-9ed9-941ee3a40011",
   "metadata": {},
   "source": [
    "# Dataset Loader Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05511d7-b5ec-46c9-bf37-8ea115dc090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5b990b-9b1e-4a2e-a368-7df486b6dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "import torchvision\n",
    "import normflows as nf\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gendis.datasets import ClusteredMultiDistrDataModule, CausalMNIST\n",
    "from gendis.model import CausalMultiscaleFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df58bcd-b79f-4dcc-9777-40d2ff0fcad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/adam2392/pytorch_data/\"\n",
    "graph_type = 'chain'\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "log_dir='./'\n",
    "intervention_types = [None, 1, 2, 3]\n",
    "hard_interventions_per_distr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154663c1-14cf-4a8c-b3e4-6d1be5639be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up transforms for each image to augment the dataset\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        nf.utils.Scale(255.0 / 256.0),  # normalize the pixel values\n",
    "        nf.utils.Jitter(1 / 256.0),  # apply random generation\n",
    "        torchvision.transforms.RandomRotation(350),  # get random rotations\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ff2d7a-97f3-4ada-91b6-985c3b1af858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating dataset: for label 0 with 5923 samples\n",
      "Color value:  torch.Size([5923, 1])\n",
      "\n",
      "\n",
      "Generating dataset: for label 0 with 5923 samples\n",
      "Color value:  torch.Size([5923, 1])\n",
      "\n",
      "\n",
      "Generating dataset: for label 0 with 5923 samples\n",
      "Color value:  torch.Size([5923, 1])\n",
      "\n",
      "\n",
      "Generating dataset: for label 0 with 5923 samples\n",
      "Color value:  torch.Size([5923, 1])\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-0-train.pt\"\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-1-train.pt\"\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-2-train.pt\"\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-3-train.pt\"\n"
     ]
    }
   ],
   "source": [
    "dataset = CausalMNIST(\n",
    "    root=root,\n",
    "    graph_type=graph_type,\n",
    "    label=0,\n",
    "    download=True,\n",
    "    train=True,\n",
    "    n_jobs=None,\n",
    "    intervention_idx=intervention_types,\n",
    "    transform=transform,\n",
    ")\n",
    "dataset.prepare_dataset(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e083effe-9aa6-4f11-bc21-84f2fdc59815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23692\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf8e20a-d996-4ba1-8356-bb47ee806155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-0-train.pt\"\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-1-train.pt\"\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-2-train.pt\"\n",
      "\n",
      "\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-3-train.pt\"\n"
     ]
    }
   ],
   "source": [
    "# now we can wrap this in a pytorch lightning datamodule\n",
    "data_module = ClusteredMultiDistrDataModule(\n",
    "    root=root,\n",
    "    graph_type=graph_type,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    "    intervention_types=intervention_types,\n",
    "    transform=transform,\n",
    "    log_dir=log_dir,\n",
    "    flatten=False,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634de3e1-1ab4-4e7d-b1a8-4dde6aab8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data_module.train_dataloader())\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b2698eb-5807-491a-8b20-82c5375210f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 28, 28])\n",
      "[torch.Size([4]), torch.Size([4]), torch.Size([4]), torch.Size([4]), torch.Size([4]), torch.Size([4]), torch.Size([4, 3])]\n",
      "tensor([1, 2, 1, 0])\n",
      "tensor([2.0000, 1.0320, 2.0000, 0.5048]) tensor([8.6310, 7.8262, 9.4272, 7.9743]) tensor([2., 0., 2., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print([x.shape for x in labels])\n",
    "print(labels[-2])\n",
    "print(labels[0], labels[2], labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257e4147-8460-421a-a317-0ee721f96889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23692\n",
      "<class 'collections.defaultdict'>\n",
      "23692\n"
     ]
    }
   ],
   "source": [
    "print(len(data_module.train_dataset.dataset.data))\n",
    "print(type(data_module.train_dataset.dataset.meta_labels))\n",
    "print(len(data_module.train_dataset.dataset.meta_labels['width']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2892c163-7cdc-4d6a-8d91-a5157f6cbea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchvision\n",
    "\n",
    "def show_imgs(imgs, title=None, row_size=4):\n",
    "    # Form a grid of pictures (we use max. 8 columns)\n",
    "    num_imgs = imgs.shape[0] if isinstance(imgs, torch.Tensor) else len(imgs)\n",
    "    is_int = imgs.dtype==torch.int32 if isinstance(imgs, torch.Tensor) else imgs[0].dtype==torch.int32\n",
    "    nrow = min(num_imgs, row_size)\n",
    "    ncol = int(math.ceil(num_imgs/nrow))\n",
    "    imgs = torchvision.utils.make_grid(imgs, nrow=nrow, pad_value=128 if is_int else 0.5)\n",
    "    np_imgs = imgs.cpu().numpy()\n",
    "    # Plot the grid\n",
    "    plt.figure(figsize=(1.5*nrow, 1.5*ncol))\n",
    "    plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4cca83-2612-4d4d-a6e2-4d471410d102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x150 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_imgs(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a5b71-2777-4530-ad75-1d708a04e795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99111139-fd94-43bd-ac7f-5b963aefb206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdrl",
   "language": "python",
   "name": "cdrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
