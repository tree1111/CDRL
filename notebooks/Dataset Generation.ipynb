{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7abc5c9c-f9ee-4cbe-8e17-64fba01626c3",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "In our analysis, we want to generate a variation of the MNIST dataset that demonstrates the settings in our paper.\n",
    "First we will consider a setting with only one type of digit (i.e. a 0). We will then demonstrate how to generate\n",
    "data according to a latent causal graph over the considered variables.\n",
    "More specifically, consider the following continuous variables:\n",
    "\n",
    "- color\n",
    "- thickness/thinness (width)\n",
    "- fractures\n",
    "- swelling\n",
    "\n",
    "Each of these will be continuously generated within a compact domain according to the causal graph:\n",
    "\n",
    "swelling <- width <- color -> fractures\n",
    "\n",
    "(also shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831ce63e-e1bc-4156-9ad6-9d1137975556",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 11.0.0 (20240428.1522)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"186pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 186.30 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 182.3,-112 182.3,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"42.14\" cy=\"-90\" rx=\"42.14\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"42.14\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">swelling</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"42.14\" cy=\"-18\" rx=\"31.9\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"42.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">width</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M42.14,-71.7C42.14,-64.41 42.14,-55.73 42.14,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"45.64,-47.62 42.14,-37.62 38.64,-47.62 45.64,-47.62\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"133.14\" cy=\"-90\" rx=\"29.86\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.14\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">color</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;B -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>C&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.16,-75.17C102.03,-65.07 84.01,-51.21 69.16,-39.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"71.65,-37.29 61.59,-33.96 67.38,-42.83 71.65,-37.29\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135.14\" cy=\"-18\" rx=\"43.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.14\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">fractures</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.63,-71.7C133.84,-64.41 134.09,-55.73 134.32,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.82,-47.71 134.61,-37.62 130.82,-47.51 137.82,-47.71\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x107e727a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a new directed graph\n",
    "dot = Digraph()\n",
    "\n",
    "# Add nodes\n",
    "dot.node('A', 'swelling')\n",
    "dot.node('B', 'width')\n",
    "dot.node('C', 'color')\n",
    "dot.node('D', 'fractures')\n",
    "\n",
    "# Add edges\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('C', 'B')\n",
    "dot.edge('C', 'D')\n",
    "\n",
    "# Render the graph\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2eb58-cae7-4ee7-a386-602c665c17aa",
   "metadata": {},
   "source": [
    "where the degree of swelling is inversely correlated with the width of the digit, which has a relationship with the color. The more red an image is the more wide they are, and the more fractures they have.\n",
    "\n",
    "Now, let us actually generate the data in steps. We will leverage the MORPHO-MNIST API to generate different widths, fractures and swelling.\n",
    "To modify the MNIST dataset to provide color, we will implement a variation of the IRM paper procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19027ec0-bca6-4776-b368-bec88bd82628",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b4a329-bd63-4b59-b4b4-02b6d534660c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdataset_utils\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torchvision/models/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torchvision/models/convnext.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torchvision/ops/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgiou_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoolers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_align\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torchvision/ops/poolers.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroi_align\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torchvision/ops/roi_align.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Union\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structured\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signpost_event\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     ConstraintViolationError,\n\u001b[1;32m     33\u001b[0m     GuardOnDataDependentSymNode,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _forward_from_src \u001b[38;5;28;01mas\u001b[39;00m original_forward_from_src\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py:63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeGuard, Source, TracingContext\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloorDiv, Mod, IsNonOverlappingAndDenseIndicator\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m try_solve\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalue_ranges\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bound_sympy, SymPyValueRangeAnalysis, ValueRanges, ValueRangeError\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/torch/utils/_sympy/functions.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzzy_and, fuzzy_not, fuzzy_or\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/sympy/__init__.py:108\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[1;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[1;32m     75\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[1;32m     76\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[1;32m    106\u001b[0m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[1;32m    109\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[1;32m    110\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[1;32m    113\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[1;32m    114\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[1;32m    134\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mntheory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (nextprime, prevprime, prime, primepi, primerange,\n\u001b[1;32m    137\u001b[0m         randprime, Sieve, sieve, primorial, cycle_length, composite,\n\u001b[1;32m    138\u001b[0m         compositepi, isprime, divisors, proper_divisors, factorint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m         continued_fraction_iterator, continued_fraction_reduce,\n\u001b[1;32m    150\u001b[0m         continued_fraction_convergents, continued_fraction, egyptian_fraction)\n",
      "File \u001b[0;32m~/miniforge3/envs/cdrl/lib/python3.10/site-packages/sympy/series/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgruntz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gruntz\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m series\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapproximants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m approximants\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresidues\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m residue\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequences\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeqPer, SeqFormula, sequence, SeqAdd, SeqMul\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets.utils as dataset_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a022180-72c6-4eba-91a4-3c441120b5e4",
   "metadata": {},
   "source": [
    "## Generate Images with different colors on the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f14ed4-18d0-4204-ab5d-0f181450a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, color_value):\n",
    "    \"\"\"Converts grayscale image to a uniform color based on a continuous value\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    colored_arr = np.zeros((h, w, 3), dtype=dtype)\n",
    "    color = plt.cm.viridis(color_value)[:3]  # Use a continuous colormap like viridis and extract RGB values\n",
    "    mask = arr > 0  # Mask to identify the digit\n",
    "    for i in range(3):\n",
    "        colored_arr[:, :, i][mask] = color[i] * 255  # Apply uniform color to the digit\n",
    "    return colored_arr.astype(dtype)\n",
    "\n",
    "def prepare_colored_mnist(root='./data', overwrite=False):\n",
    "    colored_mnist_dir = os.path.join(root, 'ColoredMNIST')\n",
    "    if not overwrite and os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
    "            and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
    "            and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
    "        print('Colored MNIST dataset already exists')\n",
    "        return\n",
    "\n",
    "    print('Preparing Colored MNIST')\n",
    "    train_mnist = datasets.MNIST(root, train=True, download=True)\n",
    "\n",
    "    train1_set = []\n",
    "    train2_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for idx, (im, label) in enumerate(train_mnist):\n",
    "        if idx % 10000 == 0:\n",
    "            print(f'Converting image {idx}/{len(train_mnist)}')\n",
    "        im_array = np.array(im)\n",
    "\n",
    "        # Normalize label to [0, 1] for continuous coloring\n",
    "        color_value = label / 9.0\n",
    "        \n",
    "#         print(idx, label,color_value)\n",
    "        colored_arr = color_grayscale_arr(im_array, color_value)\n",
    "        \n",
    "        assert False\n",
    "        if idx < 20000:\n",
    "            train1_set.append((Image.fromarray(colored_arr), label))\n",
    "        elif idx < 40000:\n",
    "            train2_set.append((Image.fromarray(colored_arr), label))\n",
    "        else:\n",
    "            test_set.append((Image.fromarray(colored_arr), label))\n",
    "\n",
    "    os.makedirs(colored_mnist_dir, exist_ok=True)\n",
    "    torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
    "    torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "    torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))\n",
    "\n",
    "def load_colored_mnist(env='train1', root='./data'):\n",
    "    colored_mnist_dir = os.path.join(root, 'ColoredMNIST')\n",
    "    print(f'Loading data from colored mnist: {colored_mnist_dir}')\n",
    "    if env in ['train1', 'train2', 'test']:\n",
    "        data_label_tuples = torch.load(os.path.join(colored_mnist_dir, env) + '.pt')\n",
    "    elif env == 'all_train':\n",
    "        data_label_tuples = torch.load(os.path.join(colored_mnist_dir, 'train1.pt')) + \\\n",
    "                            torch.load(os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "    else:\n",
    "        raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
    "    return data_label_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a5a1e-914a-4e74-863f-c1c8dfd8cadd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = '/Users/adam2392/pytorch_data/'\n",
    "overwrite = True\n",
    "\n",
    "# Prepare Colored MNIST dataset\n",
    "prepare_colored_mnist(root, overwrite=overwrite)\n",
    "\n",
    "# Load some example images\n",
    "data_label_tuples = load_colored_mnist(env='train1', root=root)\n",
    "\n",
    "# Display some example images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, (img, label) in enumerate(data_label_tuples[:5]):\n",
    "    print(np.array(img).shape)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'Label: {label}')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde9e1-724b-4a3d-a9fe-2ea853828acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,1]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 1.5))\n",
    "img = plt.imshow(a, cmap=\"viridis\")\n",
    "plt.gca().set_visible(False)\n",
    "cax = plt.axes([0.1, 0.2, 0.8, 0.6])\n",
    "plt.colorbar(orientation=\"horizontal\", cax=cax, label='Possible colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6a054-839d-47ed-9c0c-b777094cbf33",
   "metadata": {},
   "source": [
    "## Generate images with different widths using MorphoMNIST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94449988-0afb-4a47-aa4d-0807309748a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gendis.datasets.morphomnist import perturb, morpho\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd2b47-6d67-4415-a00a-32837b26d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_perturbation(image, perturbation):\n",
    "    # Convert image to binary\n",
    "    image = image.squeeze().numpy()\n",
    "    binary_image = np.array(image) > 0\n",
    "#     binary_image = np.array(image.convert('L')) > 0  # Ensure the image is in grayscale and binary\n",
    "\n",
    "    morph = morpho.ImageMorphology(binary_image)\n",
    "    perturbed_image = perturbation(morph)\n",
    "    return Image.fromarray((perturbed_image * 255).astype(np.uint8))\n",
    "\n",
    "# Define the dataset loader with perturbations\n",
    "class PerturbedMNIST(datasets.MNIST):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, perturbation=None):\n",
    "        super(PerturbedMNIST, self).__init__(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        self.perturbation = perturbation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(PerturbedMNIST, self).__getitem__(index)\n",
    "        if self.perturbation is not None:\n",
    "            img = apply_perturbation(img, self.perturbation)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        prepare_colored_mnist(root='', overwrite=False)\n",
    "    \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bed97-c65d-4f4e-a2cd-7d6ccba28eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the perturbations\n",
    "thinning = perturb.Thinning(amount=0.7)\n",
    "thickening = perturb.Thickening(amount=1.0)\n",
    "swelling = perturb.Swelling(strength=3, radius=10)\n",
    "fracture = perturb.Fracture(thickness=1.5, prune=2, num_frac=1)\n",
    "\n",
    "# Apply transformations and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "train_dataset_thinning = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=thinning)\n",
    "train_dataset_thickening = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=thickening)\n",
    "train_dataset_swelling = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=swelling)\n",
    "train_dataset_fracture = PerturbedMNIST(root=root, train=True, download=True, transform=transform, perturbation=fracture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dbf2a-b88c-4a22-8937-f807dabe66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize some examples\n",
    "def visualize_examples(dataset, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i in range(5):\n",
    "        img, label = dataset[i]\n",
    "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display examples\n",
    "# visualize_examples(train_dataset_thinning, 'Thinning Perturbation')\n",
    "# visualize_examples(train_dataset_thickening, 'Thickening Perturbation')\n",
    "# visualize_examples(train_dataset_swelling, 'Swelling Perturbation')\n",
    "visualize_examples(train_dataset_fracture, 'Fracture Perturbation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2ea65-ed9c-4fac-a7f2-01451dd03f6e",
   "metadata": {},
   "source": [
    "## Combining Color and Morpho MNIST\n",
    "\n",
    "First, we'll look at how we can combine perturbations in a VisionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75c882-fb04-4359-b556-436bab98b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_perturbation(image, perturbation):\n",
    "    # Convert image to binary\n",
    "    image = np.array(image)\n",
    "    binary_image = np.array(image) > 0\n",
    "\n",
    "    morph = morpho.ImageMorphology(binary_image)\n",
    "    perturbed_image = perturbation(morph)\n",
    "    return Image.fromarray((perturbed_image * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "# Define the dataset loader with perturbations\n",
    "class PerturbedMNIST(datasets.MNIST):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "        perturbations=None,\n",
    "        \n",
    "    ):\n",
    "        super(PerturbedMNIST, self).__init__(\n",
    "            root,\n",
    "            train=train,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            download=download,\n",
    "        )\n",
    "\n",
    "        if perturbations is not None and not isinstance(perturbations, list):\n",
    "            raise RuntimeError(\"Should be a list of perturbations\")\n",
    "        self.perturbations = perturbations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super(PerturbedMNIST, self).__getitem__(index)\n",
    "        img = img.squeeze()\n",
    "        if self.perturbations is not None:\n",
    "            for perturbation in self.perturbations:\n",
    "                img = torch.Tensor(np.array(img))\n",
    "                img = apply_perturbation(img, perturbation)\n",
    "        \n",
    "        # Normalize label to [0, 1] for continuous coloring\n",
    "        color_value = target / 9.0\n",
    "        img = color_grayscale_arr(np.array(img), color_value)\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    \n",
    "def color_grayscale_arr(arr, color_value):\n",
    "    \"\"\"Converts grayscale image to a uniform color based on a continuous value\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    colored_arr = np.zeros((h, w, 3), dtype=dtype)\n",
    "    color = plt.cm.viridis(color_value)[:3]  # Use a continuous colormap like viridis and extract RGB values\n",
    "    mask = arr > 0  # Mask to identify the digit\n",
    "    for i in range(3):\n",
    "        colored_arr[:, :, i][mask] = color[i] * 255  # Apply uniform color to the digit\n",
    "    return colored_arr.astype(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca9411-db94-4b54-b2a6-9ddb147c1fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize some examples\n",
    "def visualize_examples(dataset, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for i in range(5):\n",
    "        img, label = dataset[i]\n",
    "#         print('here...',\n",
    "#               np.array(img).shape)\n",
    "#         img = np.moveaxis(np.array(img), 0, 2)\n",
    "#         print(img.shape)\n",
    "        img = Image.fromarray(img)\n",
    "        axes[i].imshow(img, cmap='viridis')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "perturbations = [\n",
    "    perturb.Thickening(amount=0.2),\n",
    "#     perturb.Swelling(strength=1.75, radius=7),\n",
    "    perturb.Fracture(thickness=1.1, \n",
    "#                      prune=10, \n",
    "                     num_frac=2)\n",
    "]\n",
    "    \n",
    "# Apply transformations and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "train_dataset = PerturbedMNIST(root=root, train=True, download=False, transform=transform, perturbations=perturbations)\n",
    "    \n",
    "visualize_examples(train_dataset, 'All perturbations and colors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3301a-ca18-46b5-9c58-aee6fa08f9e7",
   "metadata": {},
   "source": [
    "## Full CausalMNIST on a single digit\n",
    "\n",
    "Now, we can demonstrate CausalMNIST on a single digit \"0\", where we apply color, fracture and width changes in the form of a latent causal graph among these latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0b9c40-0709-41da-bb68-87eafa706e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gendis.datasets.causalmnist import CausalMNIST\n",
    "from gendis.model import NonlinearNeuralClusteredASCMFlow, LinearNeuralClusteredASCMFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c375fe-88f0-42a4-8d0e-7f313d2e2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = 'chain'\n",
    "root = '/Users/adam2392/pytorch_data/'\n",
    "\n",
    "dataset = CausalMNIST(root=root, graph_type=graph_type, label=0, download=True, train=True, n_jobs=None, intervention_idx=)\n",
    "dataset.prepare_dataset(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab69d72-9256-4d38-b024-01403307e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.prepare_dataset(overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915575a9-cdfb-488d-bd82-c39f6c21db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in range(5):\n",
    "    img, meta_label = dataset[idx]\n",
    "    print(meta_label)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img, cmap='viridis')\n",
    "    ax.set_title(f\"Label: {meta_label}\")\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884afcf9-5214-49b1-bb6e-03e6fbb82307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numValues = 10000\n",
    "maxValue = 2.0\n",
    "skewness = -6   #Negative values are left skewed, positive values are right skewed.\n",
    "\n",
    "random = skewnorm.rvs(a = skewness / 2,loc=maxValue, \n",
    "                      scale=0.2,\n",
    "                      size=numValues)  #Skewnorm function\n",
    "\n",
    "random = random - min(random)      #Shift the set so the minimum value is equal to zero.\n",
    "random = random / max(random) * 2      #Standadize all the vlues between 0 and 1. \n",
    "# random = random * maxValue         #Multiply the standardized values by the maximum value.\n",
    "\n",
    "#Plot histogram to check skewness\n",
    "plt.hist(random,30,density=True, color = 'red', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac779126-ed9c-468a-bd22-0491744e2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "numValues = 10000\n",
    "maxValue = 10\n",
    "skewness = 1   #Negative values are left skewed, positive values are right skewed.\n",
    "\n",
    "width_intervention = stats.skewnorm.rvs(a=0, loc=2, size=(n_samples, 1))\n",
    "width_intervention = width_intervention - min(width_intervention)      #Shift the set so the minimum value is equal to zero.\n",
    "width = (torch.rand(size=(n_samples, 1)) * 2) * width_intervention\n",
    "# width = width - min(width)\n",
    "# width = width / max(width) * 2\n",
    "# random = random * maxValue         #Multiply the standardized values by the maximum value.\n",
    "width = width.squeeze()\n",
    "print(width.shape)\n",
    "#Plot histogram to check skewness\n",
    "plt.hist(width,30,density=True, \n",
    "#          color = 'red', \n",
    "         alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efed4d-e814-43b3-9ed9-941ee3a40011",
   "metadata": {},
   "source": [
    "# Dataset Loader Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05511d7-b5ec-46c9-bf37-8ea115dc090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5b990b-9b1e-4a2e-a368-7df486b6dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from gendis.datasets import ClusteredMultiDistrDataModule, CausalMNIST\n",
    "from gendis.model import NonlinearNeuralClusteredASCMFlow, LinearNeuralClusteredASCMFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df58bcd-b79f-4dcc-9777-40d2ff0fcad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/adam2392/pytorch_data/\"\n",
    "graph_type = 'chain'\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "log_dir='./'\n",
    "intervention_targets_per_distr = []\n",
    "hard_interventions_per_distr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96be647-43c7-41bd-b453-3706911df9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-0-train.pt\"\n",
      "dict_keys(['width', 'color', 'fracture_thickness', 'fracture_num_fractures', 'label', 'intervention_targets'])\n",
      "torch.Size([3, 28, 28]) {'width': tensor([0.5344]), 'color': tensor([0.3977]), 'fracture_thickness': tensor([8.9378]), 'fracture_num_fractures': tensor([1.]), 'label': 0, 'intervention_targets': [0, 0, 0]}\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-1-train.pt\"\n",
      "dict_keys(['width', 'color', 'fracture_thickness', 'fracture_num_fractures', 'label', 'intervention_targets'])\n",
      "torch.Size([3, 28, 28]) {'width': tensor([4.0281]), 'color': tensor([0.3975]), 'fracture_thickness': tensor([9.4378]), 'fracture_num_fractures': tensor([1.]), 'label': 0, 'intervention_targets': [1, 0, 0]}\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-2-train.pt\"\n",
      "dict_keys(['width', 'color', 'fracture_thickness', 'fracture_num_fractures', 'label', 'intervention_targets'])\n",
      "torch.Size([3, 28, 28]) {'width': tensor([0.4431]), 'color': tensor([0.4416]), 'fracture_thickness': tensor([10.2519]), 'fracture_num_fractures': tensor([0.]), 'label': 0, 'intervention_targets': [0, 0, 1]}\n",
      "Loading dataset from \"/Users/adam2392/pytorch_data/CausalMNIST/chain/chain-3-train.pt\"\n",
      "dict_keys(['width', 'color', 'fracture_thickness', 'fracture_num_fractures', 'label', 'intervention_targets'])\n",
      "torch.Size([3, 28, 28]) {'width': tensor([1.2834]), 'color': tensor([0.5551]), 'fracture_thickness': tensor([8.9974]), 'fracture_num_fractures': tensor([0.]), 'label': 0, 'intervention_targets': [0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "for intervention_idx in [None, 1, 2, 3]:\n",
    "    dataset = CausalMNIST(\n",
    "        root=root,\n",
    "        graph_type=graph_type,\n",
    "        label=0,\n",
    "        download=True,\n",
    "        train=True,\n",
    "        n_jobs=None,\n",
    "        intervention_idx=intervention_idx,\n",
    "    )\n",
    "    dataset.prepare_dataset(overwrite=False)\n",
    "    datasets.append(dataset)\n",
    "    \n",
    "    intervention_targets_per_distr.append(dataset.intervention_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf8e20a-d996-4ba1-8356-bb47ee806155",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ClusteredMultiDistrDataModule(\n",
    "    datasets=datasets,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=5,\n",
    "    intervention_targets_per_distr=intervention_targets_per_distr,\n",
    "    log_dir=log_dir,\n",
    "    flatten=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634de3e1-1ab4-4e7d-b1a8-4dde6aab8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23692, 2352]) 6 torch.Size([23692]) torch.Size([23692]) torch.Size([23692])\n"
     ]
    }
   ],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5abfba7-ddd8-40fa-8e13-6c7f12255483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "def generate_list(x, n_clusters):\n",
    "    quotient = x // n_clusters\n",
    "    remainder = x % n_clusters\n",
    "    result = [quotient] * (n_clusters - 1)\n",
    "    result.append(quotient + remainder)\n",
    "    return result\n",
    "\n",
    "adjacency_matrix = np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]])\n",
    "latent_dim = len(adjacency_matrix)\n",
    "seed = 1234\n",
    "max_epochs=2\n",
    "accelerator='mps'\n",
    "devices=1\n",
    "\n",
    "n_flows = 3  # number of flows to use in nonlinear ICA model\n",
    "lr_scheduler = None\n",
    "lr_min = 0.0\n",
    "lr = 1e-6\n",
    "\n",
    "# Define the model\n",
    "net_hidden_dim = 128\n",
    "net_hidden_dim_cbn = 128\n",
    "net_hidden_layers = 3\n",
    "net_hidden_layers_cbn = 3\n",
    "fix_mechanisms = False\n",
    "\n",
    "model = NonlinearNeuralClusteredASCMFlow(\n",
    "    cluster_sizes=generate_list(784*3, 3),\n",
    "    graph=adjacency_matrix,\n",
    "    intervention_targets_per_distr=intervention_targets_per_distr,\n",
    "    hard_interventions_per_distr=hard_interventions_per_distr,\n",
    "    n_flows=n_flows,\n",
    "    n_hidden_dim=net_hidden_dim,\n",
    "    n_layers=net_hidden_layers,\n",
    "    lr=lr,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    lr_min=lr_min,\n",
    "    fix_mechanisms=fix_mechanisms,\n",
    ")\n",
    "checkpoint_root_dir = f\"{graph_type}-seed={seed}\"\n",
    "checkpoint_dir = Path(checkpoint_root_dir) / \"default\"\n",
    "logger = None\n",
    "wandb = False\n",
    "check_val_every_n_epoch = 1\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    every_n_epochs=check_val_every_n_epoch,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    logger=logger,\n",
    "    devices=devices,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    check_val_every_n_epoch=check_val_every_n_epoch,\n",
    "    accelerator=accelerator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2180953c-762a-4402-b99a-d5fd0bf67bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type                                | Params\n",
      "----------------------------------------------------------------\n",
      "0 | encoder | NonparametricClusteredCausalEncoder | 44.3 M\n",
      "----------------------------------------------------------------\n",
      "44.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.3 M    Total params\n",
      "177.087   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23692, 2352]) 6 torch.Size([23692]) torch.Size([23692]) torch.Size([23692])\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/miniforge3/envs/cdrl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/Users/adam2392/miniforge3/envs/cdrl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                    | 0/2 [00:00<?, ?it/s]torch.Size([5]) torch.Size([5, 2352]) torch.Size([5]) torch.Size([5]) torch.Size([5]) torch.Size([5]) torch.Size([5])\n",
      "Sanity Checking DataLoader 0:  50%|                              | 1/2 [00:00<00:00,  1.81it/s]torch.Size([5]) torch.Size([5, 2352]) torch.Size([5]) torch.Size([5]) torch.Size([5]) torch.Size([5]) torch.Size([5])\n",
      "                                                                                                                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/miniforge3/envs/cdrl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                             | 2/4289 [00:02<1:46:30,  1.49s/it, loss=7.82e+06]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam2392/miniforge3/envs/cdrl/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd01b92f-4ce4-4480-bb0f-7e6ab16ca2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "results_dir = Path('/Users/adam2392/pytorch_data/CausalMNIST/results/')\n",
    "results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "torch.save(model, results_dir / f'trained-model-{graph_type}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf40ff-a365-4a8e-a7d5-0ab8c68fa148",
   "metadata": {},
   "source": [
    "# Demo Trained Model Reproducing Images\n",
    "\n",
    "Here, we are interested in qualitatively analyzing the trained model. This will entail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fc09b9f-1ea4-426a-b66b-b761859a9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NonparametricClusteredCausalEncoder(\n",
      "  (q0): NonparametricClusteredCausalDistribution(\n",
      "    (q0): MultiEnvBaseDistribution()\n",
      "    (flows): ModuleList(\n",
      "      (0-2): 3 x AutoregressiveRationalQuadraticSpline(\n",
      "        (mprqat): MaskedPiecewiseRationalQuadraticAutoregressive(\n",
      "          (autoregressive_net): MADE(\n",
      "            (preprocessing): Identity()\n",
      "            (initial_layer): MaskedLinear(in_features=2352, out_features=128, bias=True)\n",
      "            (blocks): ModuleList(\n",
      "              (0-2): 3 x MaskedResidualBlock(\n",
      "                (linear_layers): ModuleList(\n",
      "                  (0-1): 2 x MaskedLinear(in_features=128, out_features=128, bias=True)\n",
      "                )\n",
      "                (activation): ReLU()\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (final_layer): MaskedLinear(in_features=128, out_features=54096, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flows): ModuleList(\n",
      "    (0-2): 3 x AutoregressiveRationalQuadraticSpline(\n",
      "      (mprqat): MaskedPiecewiseRationalQuadraticAutoregressive(\n",
      "        (autoregressive_net): MADE(\n",
      "          (preprocessing): Identity()\n",
      "          (initial_layer): MaskedLinear(in_features=2352, out_features=128, bias=True)\n",
      "          (blocks): ModuleList(\n",
      "            (0-2): 3 x MaskedResidualBlock(\n",
      "              (linear_layers): ModuleList(\n",
      "                (0-1): 2 x MaskedLinear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "              (activation): ReLU()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_layer): MaskedLinear(in_features=128, out_features=54096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print(model)\n",
    "print(model.encoder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7c333-8680-4112-ba4b-a6d511702425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdrl",
   "language": "python",
   "name": "cdrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
